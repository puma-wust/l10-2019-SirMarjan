{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro.optim as optim\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "from tqdm import tqdm_notebook as tqdm  \n",
    "import seaborn as sns\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoDelta\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, TracePredictive\n",
    "import pandas as pd\n",
    "\n",
    "from pyro import poutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from numpy import pi\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_name, delete_columns=None, y_column=-1):\n",
    "    if delete_columns is None:\n",
    "        delete_columns = []\n",
    "    df = pd.read_csv(file_name, header=None)\n",
    "    matrix = df.to_numpy()\n",
    "    result_matrix = np.delete(matrix, delete_columns, axis=1)\n",
    "    result_matrix = np.delete(result_matrix, y_column, axis=1)\n",
    "    result_matrix = np.hstack((result_matrix, matrix[:, y_column][:, np.newaxis]))\n",
    "    return result_matrix\n",
    "\n",
    "\n",
    "def split_data(raw_data_matrix, test_set_percent, shuffle):\n",
    "    if shuffle:\n",
    "        process_data = np.random.permutation(raw_data_matrix)\n",
    "    else:\n",
    "        process_data = raw_data_matrix\n",
    "    split_index = int(process_data.shape[0] * test_set_percent)\n",
    "    return process_data[:split_index, :], process_data[split_index:, :]\n",
    "\n",
    "\n",
    "def extract_attributes_label(data_matrix) -> dict:\n",
    "    class_vector, class_labels = pd.factorize(data_matrix[:, -1])\n",
    "    data_matrix = np.vstack((data_matrix[:, :-1].T, class_vector)).T.astype(float)\n",
    "    return {\"x_y\": data_matrix, \"x\": data_matrix[:, :-1], \"y\": data_matrix[:, -1], \"y_labels\": np.vstack(\n",
    "        (np.arange(class_labels.size), class_labels)).T, \"y_values\": np.arange(class_labels.size)}\n",
    "\n",
    "\n",
    "def positive(predict_vector, class_vector):\n",
    "    return np.count_nonzero((predict_vector - class_vector) == 0) / predict_vector.shape[0]\n",
    "\n",
    "def calc_metric(y_true: np.ndarray, y_pred: np.ndarray, average: str = \"macro\"):\n",
    "    return Metric(\n",
    "        accuracy=sm.accuracy_score(y_true, y_pred),\n",
    "        precision=sm.precision_score(y_true, y_pred, average=average),\n",
    "        recall=sm.recall_score(y_true, y_pred, average=average),\n",
    "        f1=sm.f1_score(y_true, y_pred, average=average)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = (load(\"data/iris.csv\"), \"iris\")\n",
    "extracted_data = extract_attributes_label(data_matrix[0])\n",
    "\n",
    "x_data = torch.from_numpy(extracted_data[\"x\"])\n",
    "y_data = torch.from_numpy(extracted_data[\"y\"])\n",
    "classes_nbr = len(y_data.unique())\n",
    "features_nbr = len(x_data[1,:])\n",
    "count = len(y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBModel(nn.Module):\n",
    "    def __init__(self,classes,features):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.features = features\n",
    "        \n",
    "        self.register_parameter(\n",
    "            \"means\",\n",
    "            nn.Parameter(torch.zeros(self.classes, self.features))\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            \"variances\",\n",
    "            nn.Parameter(torch.ones(self.classes, self.features))\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            \"classes_priors\",\n",
    "            nn.Parameter(torch.ones(self.classes) * (1/self.classes))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,np.newaxis,:]\n",
    "        return (torch.sum(- 0.5 * torch.log(2. * pi * self.variances)\n",
    "                - (x - self.means)**2 / torch.abs(self.variances) / 2, dim=-1)\n",
    "                + torch.log(self.classes_priors))\n",
    "    \n",
    "nbModel = NBModel(classes_nbr,features_nbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_model2(x_data, y_data):\n",
    "    \n",
    "    means = pyro.distributions.Normal(loc=torch.zeros_like(nbModel.means), scale=torch.ones_like(nbModel.means)).to_event(2)\n",
    "    variances = pyro.distributions.InverseGamma(concentration=torch.ones_like(nbModel.variances),rate=torch.ones_like(nbModel.variances)).to_event(2)\n",
    "    classes_priors = pyro.distributions.Dirichlet(concentration=torch.ones_like(nbModel.classes_priors))\n",
    "    \n",
    "    priors = {'means': means, 'variances': variances, \"classes_priors\": classes_priors}\n",
    "    lifted_module = pyro.random_module(\"module\", nbModel, priors)\n",
    "    lifted_nb_model = lifted_module()\n",
    "    \n",
    "    #with pyro.plate(\"map_cl\", len(x_data)):\n",
    "        \n",
    "    \n",
    "    with pyro.plate(\"map\", len(x_data)):\n",
    "        class_prob = lifted_nb_model(x_data)\n",
    "        pyro.sample(\"obs_cl\",pyro.distributions.Categorical(probs=lifted_nb_model.classes_priors),obs=y_data)\n",
    "        pyro.sample(\"obs\",pyro.distributions.Categorical(logits=class_prob),obs=y_data)\n",
    "        return class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_guide2(x_data, y_data):\n",
    "    means_m = pyro.param(\"means_m\",torch.zeros_like(nbModel.means))\n",
    "    means_s = pyro.param(\"means_s\",torch.ones_like(nbModel.means), constraint=constraints.positive)\n",
    "    variances_c = pyro.param(\"variances_a\", torch.ones_like(nbModel.variances), constraint=constraints.positive)\n",
    "    variances_r = pyro.param(\"variances_b\", torch.ones_like(nbModel.variances), constraint=constraints.positive)\n",
    "    classes_priors_c = pyro.param(\"classes_priors_c\", torch.ones_like(nbModel.classes_priors)/len(nbModel.classes_priors), constraint=constraints.positive)\n",
    "    \n",
    "    means = pyro.distributions.Normal(loc=means_m, scale=means_s).to_event(2)\n",
    "    variances = pyro.distributions.InverseGamma(concentration=variances_c,rate=variances_r).to_event(2)\n",
    "    classes_priors = pyro.distributions.Dirichlet(concentration=classes_priors_c)\n",
    "    \n",
    "    priors = {'means': means, 'variances': variances, \"classes_priors\": classes_priors}\n",
    "    lifted_module = pyro.random_module(\"module\", nbModel, priors)\n",
    "    return lifted_module()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params):\n",
    "    step_data = { \"loss\":[] }\n",
    "    for param in  params:\n",
    "        step_data[param]=[]\n",
    "    pyro.clear_param_store()\n",
    "    num_iterations=5_000\n",
    "    model = nb_model2\n",
    "    guide = nb_guide2\n",
    "    optim = pyro.optim.Adam({\"lr\": 0.01})\n",
    "    svi = pyro.infer.SVI(model, guide, optim, loss=pyro.infer.Trace_ELBO(), num_samples=count)\n",
    "    for j in tqdm(range(num_iterations)):\n",
    "        loss = svi.step(x_data, y_data.squeeze(-1))\n",
    "        step_data[\"loss\"].append(loss)\n",
    "        for param in params:\n",
    "            step_data[param].append(pyro.param(param).clone())\n",
    "        if j%1000 == 0:\n",
    "            print(f\"{j}:{loss}\")\n",
    "    return (model, svi, step_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a13b1f66c20499e8df631a3c14cdddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:3559.1796263065194\n",
      "1000:275.5432863164556\n",
      "2000:267.9606270005756\n",
      "3000:237.71254793660043\n",
      "4000:254.24414906504435\n"
     ]
    }
   ],
   "source": [
    "probabilistic_model, svi, data = train(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f4da0c19d29b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evidence lower bound (ELBO)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(data[\"loss\"])\n",
    "plt.title(\"evidence lower bound (ELBO)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "plt.plot([data.detach().numpy() for data in data[\"auto_loc\"]])\n",
    "plt.title(\"auto_loc\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loc\")\n",
    "plt.show()\n",
    "plt.plot([data.detach().numpy() for data in data[\"auto_scale\"]])\n",
    "plt.title(\"auto_scale\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"scale\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_model(x_data, y_data):\n",
    "    model_result=probabilistic_model(x_data, y_data)\n",
    "    pyro.sample(\"prediction\", pyro.distributions.Delta(model_result))\n",
    "    \n",
    "posterior = svi.run(x_data, y_data)\n",
    "\n",
    "trace_pred = TracePredictive(wrapped_model,\n",
    "                             posterior,\n",
    "                             num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred = trace_pred.run(x_data, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EmpiricalMarginal(post_pred,[\"prediction\"])\n",
    "prob = em._get_samples_and_weights()[0].detach().cpu().numpy()\n",
    "prob = np.squeeze(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.argmax(prob,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive(result,y_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "em2=EmpiricalMarginal(post_pred,[\"module$$$classes_priors\"])\n",
    "prob2 = np.squeeze(em2._get_samples_and_weights()[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31805685, 0.36316461, 0.31877854])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numpy\n",
    "    actual = numpy.array(actual)\n",
    "    predicted = numpy.array(predicted)\n",
    "\n",
    "    # calculate the confusion matrix; labels is numpy array of classification labels\n",
    "    cm = numpy.zeros((len(labels), len(labels)))\n",
    "    for a, p in zip(actual, predicted):\n",
    "        cm[a][p] += 1\n",
    "\n",
    "    # also get the accuracy easily with numpy\n",
    "    accuracy = (actual == predicted).sum() / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(y_data.numpy(),result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1049, 0.1010, 0.1299, 0.3575, 0.3067])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The parameter concentration has invalid values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3df91f7c7cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcentration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/puma/lib/python3.7/site-packages/torch/distributions/dirichlet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, concentration, validate_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcentration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcentration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcentration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcentration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDirichlet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/puma/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter concentration has invalid values"
     ]
    }
   ],
   "source": [
    "pyro.distributions.Dirichlet(concentration=torch.tensor([0.1,-1.,1.])).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
